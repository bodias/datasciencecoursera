---
title: "Milestone Report - Word Prediction"
author: "Braian Dias"
date: "October 28, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The main purpose of this report is to present an approach to the problem of predicting the next word given n-first words in a text. This could be used, for instance, to predict the next word in a smart keyboard that we naturally use in our everyday lives.

To do so, a large corpus of text documents have been used, containing text from newspappers, blogs and Twitter in four different languages: English, German, Russian and Finnish.

The model chosen to accomplish the task was the "n-gram" **URL** model, which according to Wikipedia, is a type of probabilistic languague model for predicting the next item in such a sequence in the form of a (n-1)-order Markov Model.

In the next sections of the document it will be presented the basic structure of the raw data, how it could be transformed to be suitable for use in a predictive text model and the future plans to address the text prediction problem.

# Loading data

The [full dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) is a compressed file of approximatedly 574MB, containing raw text from newspappers, blogs and Twitter in four different languages: English, German, Russian and Finnish. The English file with newspapper texts, for instance, is 205mb uncompressed. Therefore, to be able to run this report in a small fraction of time, a sample of the data will be used.
The R function to create a sample takes as input the file name, the number of lines that would be read, and the probability of reading a line (where 1 means all lines would be read, 0.5 means only 50% of the the lines will be read, and so on). The function itself will not be shown here for the sake of simplicity, but it can be seen in the file "functions.R", function "sample_text_file".
For demonstration purposes, an one third sample of the English texts will be used in this report.

TODO - profanity filter.

```{r loading_data, echo=TRUE, warning=FALSE, message=FALSE}
#load basic libraries for text handling
library(tm)
library(tokenizers)

#read sample files previously generated
en_US.blogs.sample <- readLines("sample/en_US/en_US.blogs.txt")
en_US.news.sample <- readLines("sample/en_US/en_US.news.txt")
en_US.twitter.sample <- readLines("sample/en_US/en_US.twitter.txt")

all_texts <- c(en_US.blogs.sample,en_US.news.sample,en_US.twitter.sample)

#doc <- Corpus(VectorSource(all_texts))
```


After reading the data, some transformations and cleaning processes were applied :

* Map "/","@" and "|" to white space
* Make all words lowercase
* Remove numbers
* Remove extra white spaces

```{r cleaning, echo=TRUE, warning=FALSE, message=FALSE}
all_texts <- gsub("/", " ", all_texts)
all_texts <- gsub("@", " ", all_texts)
all_texts <- gsub("_{2,}", " ", all_texts)
all_texts <- gsub("\\|", " ", all_texts)
all_texts <- tolower(all_texts)
all_texts <- gsub("(\\s+[0-9]+\\s+)", " ", all_texts)
all_texts <- gsub("\\s+", " ", all_texts)
```


The next step would be tokenize the tidy raw texts into words, to extract features from data, such as frequency and n-grams.


```{r sampling,  echo=TRUE, warning=FALSE, message=FALSE}
tokens <- unlist(tokenize_words(all_texts))
tokens.df <- data.frame(table(tokens))
names(tokens.df) <- c('word','frequency')
#make a copy of the dataset ordered by frequency to easily extract the most frequent words later 
tokens.sorted <- tokens.df[order(-tokens.df$frequency),]
```

# Exploratory Data Analysis

After loading and transforming the raw data, some simple statistics of the data are presented below to give a better understand of the data.

## Summary statistics

```{r data_transformation}
summary(tokens.sorted)

head(tokens.sorted,10)

tail(tokens.sorted,10)

barplot(tokens.sorted[1:10,]$frequency, las = 2, names.arg = tokens.sorted[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")


least_frequent_words <- tokens.sorted[tokens.sorted$frequency <= 50,"word"]
all_texts_freq <- all_texts
i <- 1
step <- 50
while (i <= length(least_frequent_words)){
  all_texts_freq <- removeWords(all_texts_freq,least_frequent_words[i:(i+step)])
  i <- i+step+1
}

```

We can see that, based on the output of the **summary** function :

* There are a lot of characters that doesn't carry any information, such as ἄ,ʚ,で, and also foreign characters. However, they appear with a low frequency.
* Most of the high frequent words are above the 3rd quantile, which leads to a conclusion that the most frequent words represents a small percentage of the data (less than 25%)

Looking at the data presented by the function **head** and the **barplot** above is possible to see the ten most frequent words. The **tail** function displays the 10-least frequent words.

Below it's a usefull representation of the words, called wordcloud. It displays the 100-most frequent words.

```{r wordcloud}
library(wordcloud)

set.seed(1234)
wordcloud(words = tokens.sorted$word, freq = tokens.sorted$frequency, min.freq = 10000,
           max.words=100, random.order=FALSE, rot.per=0.35, 
           colors=brewer.pal(8, "Dark2"))
```

## Findings

Data is indeed very noise, containing strange characters and even foreign characters. Also, most of the words appear with a very low frequency. That can be seen by the distribution of the frequencies, which says that at least 75% of the words appear with a frequency lower than 4.  

# Next steps
By having the number of occurences of each 3-gram throughout the English news texts, it's possible to determine the probability of each 3-gram to occur. With this probability, it's possible to predict the next most likely word to occur in a sentence.

Having said that, the next steps would be :
1. Follow the 3-gram approach and create an algorithm to predict the next word.
2. Use the algorithm to create a Shiny app that could be used for everyone to predict the next word given a text.



some preprocessing is needed in order to store the text in suitable format to better understand the distribution and relationship between words in the text. 
Text will then be stored in a way to fit a 3-gram model, so it will be a three words pair.